{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMJ7T3xIl5AQ",
        "colab_type": "code",
        "outputId": "48020c0c-3f57-42e7-d1d4-33b8278315c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math, re, os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import glob\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9b9ShNBmBtf",
        "colab_type": "code",
        "outputId": "c81ccedd-1d34-4b37-d48e-724a52125348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHX4ilpxl5AX",
        "colab_type": "code",
        "outputId": "5d5f9b86-380c-4186-8fbe-8a0a4f50c6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8AK6OVBl5Ac",
        "colab_type": "code",
        "outputId": "dc894fb4-0564-44ce-a588-43aaee6da537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import random\n",
        "training_filenames = glob.glob(\"/content/drive/My Drive/tfrecords/*\")\n",
        "BATCH_SIZE = 2\n",
        "random.shuffle(training_filenames)\n",
        "print(training_filenames)\n",
        "training_filenames=['/content/drive/My Drive/tfrecords/patches13.tfrecord', '/content/drive/My Drive/tfrecords/patches14.tfrecord', '/content/drive/My Drive/tfrecords/patches15.tfrecord','/content/drive/My Drive/tfrecords/patches16.tfrecord']\n",
        "print(training_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rppWGNRAmpja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_image(image_data,height,width):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    image = tf.reshape(image, [height,width, -1]) # explicit size needed for TPU\n",
        "    return image\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"img1\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"img2\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"img3\": tf.io.FixedLenFeature([], tf.string),\n",
        "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'width': tf.io.FixedLenFeature([], tf.int64)\n",
        "        # shape [] means single element\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    height = tf.cast(example['height'], tf.int32)\n",
        "    width = tf.cast(example['width'], tf.int32)\n",
        "    img1 = decode_image(example['img1'],height,width)\n",
        "    img2 = decode_image(example['img2'],height,width)\n",
        "    img3 = decode_image(example['img3'],height,width)\n",
        "\n",
        "    return img1,img2,img3,height,width"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygB3By-Wl5Ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_labeled_tfrecord , num_parallel_calls=AUTO)\n",
        "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrETMI4Al5Al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_dataset(filenames):\n",
        "    dataset = load_dataset(filenames, labeled=True)\n",
        "    dataset = dataset.map(prepare_dataset)\n",
        "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLPRzulWl5Ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(img1,img2,img3,height,width):\n",
        "    fraction = 0.8533333\n",
        "    img1 = tf.image.central_crop(img1, central_fraction = fraction)\n",
        "    img3 = tf.image.central_crop(img3, central_fraction = fraction)\n",
        "    img2 = tf.image.central_crop(img2, central_fraction = fraction)\n",
        "    return tf.concat([img1,img3],axis = -1),img2\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzeBD0Ocl5Au",
        "colab_type": "code",
        "outputId": "9fcddf56-f40d-4777-9608-c31108eb3e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Training data shapes:\")\n",
        "for img1, img2 in get_training_dataset(training_filenames).take(3):\n",
        "    print(img1.numpy().shape, img2.numpy().shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oRp7IfP5tIM",
        "colab_type": "code",
        "outputId": "e88d7644-bd05-4a6e-ef18-cb84296c09e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        }
      },
      "source": [
        "for img1,img2 in get_training_dataset(training_filenames).take(1):\n",
        "      print(img1.numpy().shape, img2.numpy().shape)\n",
        "image1 = img1.numpy()[:,:,:,0:3]\n",
        "image2 = img2.numpy()\n",
        "image3 = img1.numpy()[:,:,:,3:6]\n",
        "plt.figure()\n",
        "plt.imshow(image1[0,:,:,:])\n",
        "plt.figure()\n",
        "plt.imshow(image2[0,:,:,:])\n",
        "plt.figure()\n",
        "plt.imshow(image3[0,:,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnvICNb8l5Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_training_examples = 2500*13\n",
        "steps_per_epochs = 2500*4//BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8wm5zB-l5A3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_module(x,filters,conv_filter_size,stride,padding='same'):\n",
        "    x = tf.keras.layers.Conv2D(filters,conv_filter_size,strides = stride,padding = padding,activation = 'relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters,conv_filter_size,strides = stride,padding = padding,activation = 'relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters,conv_filter_size,strides = stride,padding = padding,activation = 'relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75fGuMXol5A8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upsample_module(x,filters,conv_filter_size,stride,upsample_size = (2,2),padding='same'):\n",
        "    x = tf.keras.layers.UpSampling2D(size = upsample_size,interpolation = 'bilinear')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters,conv_filter_size,strides = stride,padding = padding,activation = 'relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAczkKQ-l5BA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generating_kernel(x,kernel_dimension , conv_filter_size, stride, padding, upsample_size):\n",
        "    x = tf.keras.layers.Conv2D(filters = kernel_dimension,kernel_size = conv_filter_size, strides = stride, padding = padding, activation = 'relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters = kernel_dimension,kernel_size = conv_filter_size, strides = stride, padding = padding, activation = 'relu')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters = kernel_dimension,kernel_size = conv_filter_size, strides = stride, padding = padding, activation = 'relu')(x)\n",
        "    x = tf.keras.layers.UpSampling2D(size = upsample_size,interpolation = 'bilinear')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters = kernel_dimension,kernel_size = conv_filter_size, strides = stride, padding = padding, activation = 'relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iRTF3BSZl5BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# linear_layer = Linear(32)  # At instantiation, we don't know on what inputs this is going to get called\n",
        "# y = linear_layer(np.zeros(42).reshape((6,7)),|np.zeros(42).reshape((7,6)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7YC1zJTl5BL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT0gd3Rfl5BP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rkimF2KWl5BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class SeparableConvolutionSlow(tf.keras.layers.Layer):\n",
        "#     def __init__(self):\n",
        "#         super(SeparableConvolutionSlow, self).__init__()\n",
        "        \n",
        "\n",
        "#     def build(self, input_shape):\n",
        "        \n",
        "#         self.outputs = tf.Variable(initial_value=tf.zeros([BATCH_SIZE,128,128,3]),trainable=False)\n",
        "        \n",
        "    \n",
        "#     def call(self, im, vertical, horizontal):\n",
        "#         n_b = im.shape[0]\n",
        "#         n_channels = im.shape[-1]\n",
        "#         m = im.shape[1]\n",
        "#         m_out = m - 51 + 1\n",
        "        \n",
        "#         assert im.shape[1] == im.shape[2]\n",
        "#         assert vertical.shape[0] == horizontal.shape[0] == n_b\n",
        "#         assert vertical.shape[-1] == horizontal.shape[-1] == 51\n",
        "#         assert vertical.shape[1] == horizontal.shape[1] == vertical.shape[2] == horizontal.shape[2] == m_out\n",
        "\n",
        "#         self.outputs =  tf.Variable(lambda : tf.random.truncated_normal([BATCH_SIZE,128,128,3]),trainable = False)\n",
        "#         return _sep_conv_worker(im, horizontal, vertical,n_b, self.outputs)  \n",
        "    \n",
        "    \n",
        "    \n",
        "# def local_separable_conv_2d(im, horizontal, vertical, output=None):\n",
        "\n",
        "#     n_channels = im.shape[-1]\n",
        "#     m = im.shape[1]\n",
        "#     m_out = m - 51 + 1\n",
        "#     output = tf.Variable(lambda : tf.random.truncated_normal([m_out, m_out,n_channels]))\n",
        "#     for row in range(m_out):\n",
        "#         for col in range(m_out):\n",
        "#             sub_patch = im[row:row + 51, col:col + 51,:]\n",
        "# #             print(sub_patch.shape)\n",
        "#             local_horiz = tf.reshape(horizontal[row, col,:],[1,-1])\n",
        "# #             print(local_horiz.shape)\n",
        "#             local_vert = tf.reshape(vertical[row, col,:],[-1,1])\n",
        "# #             print(local_vert.shape)\n",
        "#             kernel = tf.math.multiply(local_horiz,local_vert)\n",
        "#             print(tf.reduce_sum(tf.reduce_sum((sub_patch * tf.expand_dims(kernel,-1)),axis = 0),axis = 0).shape)\n",
        "#             output[row, col,:].assign(tf.reduce_sum(tf.reduce_sum((sub_patch * tf.expand_dims(kernel,-1)),axis = 0),axis = 0))\n",
        "#             del local_horiz\n",
        "#             del local_vert\n",
        "#             del kernel\n",
        "#             del sub_patch\n",
        "            \n",
        "#     return output\n",
        "\n",
        "\n",
        "\n",
        "# def _sep_conv_worker(im, horizontal, vertical,batch_size, outputs):\n",
        "#     n_b = batch_size\n",
        "#     if n_b == None:\n",
        "#         n_b = 0\n",
        "#     for b in range(int(n_b)):\n",
        "#         local_separable_conv_2d(im[b], horizontal[b], vertical[b], output=outputs[b])\n",
        "        \n",
        "#     return outputs\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujLo-oxl5BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class SeparableConvolutionSlow(tf.keras.layers.Layer):\n",
        "#     def __init__(self):\n",
        "#         super(SeparableConvolutionSlow, self).__init__()\n",
        "        \n",
        "\n",
        "#     def build(self, input_shape):\n",
        "        \n",
        "#         self.outputs = tf.Variable(initial_value=tf.zeros([BATCH_SIZE,128,128,3]),trainable=False)\n",
        "        \n",
        "    \n",
        "#     def call(self, im, vertical, horizontal):\n",
        "#         n_b = im.shape[0]\n",
        "#         n_channels = im.shape[-1]\n",
        "#         m = im.shape[1]\n",
        "#         m_out = m - 51 + 1\n",
        "        \n",
        "#         assert im.shape[1] == im.shape[2]\n",
        "#         assert vertical.shape[0] == horizontal.shape[0] == n_b\n",
        "#         assert vertical.shape[-1] == horizontal.shape[-1] == 51\n",
        "#         assert vertical.shape[1] == horizontal.shape[1] == vertical.shape[2] == horizontal.shape[2] == m_out\n",
        "\n",
        "#         return local_separable_conv_2d(im, horizontal, vertical)  \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "# def local_separable_conv_2d(im, horizontal, vertical):\n",
        "#     output_list = []\n",
        "#     n_channels = im.shape[-1]\n",
        "#     m = im.shape[1]\n",
        "#     m_out = m - 51 + 1\n",
        "#     for row in range(m_out):\n",
        "#         for col in range(m_out):\n",
        "#             sub_patch = im[:,row:row + 51, col:col + 51,:]\n",
        "# #             print(sub_patch.shape)\n",
        "#             local_horiz = tf.expand_dims(horizontal[:,row, col,:],-2)\n",
        "# #             print(local_horiz.shape)\n",
        "#             local_vert = tf.expand_dims(vertical[:,row, col,:],-1)\n",
        "# #             print(local_vert.shape)\n",
        "# #             kernel = tf.math.multiply(local_horiz,local_vert)\n",
        "#             print(tf.reduce_sum(tf.reduce_sum((sub_patch * tf.expand_dims(local_horiz*local_vert,-1)),axis = 1),axis = 1).shape)\n",
        "#             output_list.append(tf.reduce_sum(tf.reduce_sum((sub_patch * tf.expand_dims(local_horiz*local_vert,-1)),axis = 1),axis = 1))\n",
        "\n",
        "            \n",
        "#     return tf.reshape(tf.stack(output_list,axis = 1),(-1,128,128,3))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a50rNRUl5Bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class SeparableConvolutionSlow(tf.keras.layers.Layer):\n",
        "#     def __init__(self):\n",
        "#         super(SeparableConvolutionSlow, self).__init__()\n",
        "        \n",
        "\n",
        "        \n",
        "    \n",
        "#     def call(self, im, vertical, horizontal):\n",
        "#         n_b = im.shape[0]\n",
        "#         n_channels = im.shape[-1]\n",
        "#         m = im.shape[1]\n",
        "#         m_out = m - 51 + 1\n",
        "        \n",
        "#         assert im.shape[1] == im.shape[2]\n",
        "#         assert vertical.shape[0] == horizontal.shape[0] == n_b\n",
        "#         assert vertical.shape[-1] == horizontal.shape[-1] == 51\n",
        "#         assert vertical.shape[1] == horizontal.shape[1] == vertical.shape[2] == horizontal.shape[2] == m_out\n",
        "\n",
        "#         return local_separable_conv_2d(im, horizontal, vertical)  \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "# def local_separable_conv_2d(im, horizontal, vertical):\n",
        "#     output_list = []\n",
        "#     n_channels = im.shape[-1]\n",
        "#     m = im.shape[1]\n",
        "#     m_out = m - 51 + 1\n",
        "#     image_patches = tf.reshape(tf.image.extract_patches(im,sizes = [1,51,51,1],strides = [1,1,1,1],rates = [1,1,1,1],padding = 'VALID'),(-1,128,128,51,51,3))\n",
        "#     output_kernels = tf.expand_dims(tf.math.multiply(tf.expand_dims(horizontal,-2),tf.expand_dims(vertical,-1)),-1)\n",
        "#     output_images = tf.reduce_sum(tf.reduce_sum(image_patches*output_kernels,axis = -2),axis = -2)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "# #     for row in range(m_out):\n",
        "# #         for col in range(m_out):\n",
        "# #             sub_patch = im[:,row:row + 51, col:col + 51,:]\n",
        "# # #             print(sub_patch.shape)\n",
        "# #             local_horiz = tf.expand_dims(horizontal[:,row, col,:],-2)\n",
        "# # #             print(local_horiz.shape)\n",
        "# #             local_vert = tf.expand_dims(vertical[:,row, col,:],-1)\n",
        "# # #             print(local_vert.shape)\n",
        "# # #             kernel = tf.math.multiply(local_horiz,local_vert)\n",
        "# #             print(tf.reduce_sum(tf.reduce_sum((sub_patch * tf.expand_dims(local_horiz*local_vert,-1)),axis = 1),axis = 1).shape)\n",
        "# #             output_list.append(tf.reduce_sum(tf.reduce_sum((sub_patch * tf.expand_dims(local_horiz*local_vert,-1)),axis = 1),axis = 1))\n",
        "\n",
        "            \n",
        "#     return output_images\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlAkThYOl5Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        " \n",
        "    \n",
        "    return tf.norm(tf.norm(y_true-y_pred, ord=1, axis=(1,2)),axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB0Hskpyl5Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(input_shape = (128,128,6)):\n",
        "    conv_filter_size = (3, 3)\n",
        "    stride = (1, 1)\n",
        "    padding = 'same'\n",
        "    upsample_size = (2,2)\n",
        "    kernel_dimension = 51\n",
        "    x_input = tf.keras.Input(input_shape)\n",
        "    x = x_input\n",
        "    pad_dimension = kernel_dimension//2\n",
        "    i1 = x[:,:,:,0:3]\n",
        "    i2 = x[:,:,:,3:6]\n",
        "    i1 = tf.pad(i1,[[0,0],[pad_dimension,pad_dimension],[pad_dimension,pad_dimension],[0,0]])\n",
        "    i2 = tf.pad(i2,[[0,0],[pad_dimension,pad_dimension],[pad_dimension,pad_dimension],[0,0]])\n",
        "    AvgPooling = tf.keras.layers.AveragePooling2D()\n",
        "    \n",
        "    x = conv_module(x,32,conv_filter_size,stride,padding)\n",
        "    x = AvgPooling(x)\n",
        "    \n",
        "    \n",
        "    x_64 = conv_module(x,64,conv_filter_size,stride,padding)\n",
        "    x_128 = AvgPooling(x_64)\n",
        "    \n",
        "    \n",
        "    x_128 = conv_module(x_128,128,conv_filter_size,stride,padding)\n",
        "    x_256 = AvgPooling(x_128)\n",
        "    \n",
        "    x_256 = conv_module(x_256,256,conv_filter_size,stride,padding)\n",
        "    x_512 = AvgPooling(x_256)\n",
        "    \n",
        "    x_512 = conv_module(x_512,512,conv_filter_size,stride,padding)\n",
        "    x = AvgPooling(x_512)\n",
        "    \n",
        "    x = conv_module(x,512,conv_filter_size,stride,padding)\n",
        "    \n",
        "    \n",
        "    \n",
        "    x = upsample_module(x,512,conv_filter_size,stride,upsample_size,padding)\n",
        "    x += x_512\n",
        "    x = conv_module(x,256,conv_filter_size,stride,padding)\n",
        "    \n",
        "    x = upsample_module(x,256,conv_filter_size,stride,upsample_size,padding)\n",
        "    x += x_256\n",
        "    x = conv_module(x,128,conv_filter_size,stride,padding)\n",
        "\n",
        "    x = upsample_module(x,128,conv_filter_size,stride,upsample_size,padding)\n",
        "    x += x_128\n",
        "    x = conv_module(x,64,conv_filter_size,stride,padding)\n",
        "    \n",
        "    x = upsample_module(x,64,conv_filter_size,stride,upsample_size,padding)\n",
        "    x += x_64\n",
        "#     print(x)\n",
        "    \n",
        "    k1h = generating_kernel(x,kernel_dimension , conv_filter_size, stride, padding, upsample_size)\n",
        "    k1v = generating_kernel(x,kernel_dimension , conv_filter_size, stride, padding, upsample_size)\n",
        "    k2h = generating_kernel(x,kernel_dimension , conv_filter_size, stride, padding, upsample_size)\n",
        "    k2v = generating_kernel(x,kernel_dimension , conv_filter_size, stride, padding, upsample_size)\n",
        "    image_patches1 = tf.reshape(tf.image.extract_patches(i1,sizes = [1,51,51,1],strides = [1,1,1,1],rates = [1,1,1,1],padding = 'VALID'),(-1,128,128,51,51,3))\n",
        "    output_kernels1 = tf.expand_dims(tf.math.multiply(tf.expand_dims(k1h,-2),tf.expand_dims(k1v,-1)),-1)\n",
        "    output_images1 = tf.reduce_sum(tf.reduce_sum(image_patches1*output_kernels1,axis = -2),axis = -2)\n",
        "    \n",
        "    \n",
        "    image_patches2 = tf.reshape(tf.image.extract_patches(i2,sizes = [1,51,51,1],strides = [1,1,1,1],rates = [1,1,1,1],padding = 'VALID'),(-1,128,128,51,51,3))\n",
        "    output_kernels2 = tf.expand_dims(tf.math.multiply(tf.expand_dims(k2h,-2),tf.expand_dims(k2v,-1)),-1)\n",
        "    output_images2 = tf.reduce_sum(tf.reduce_sum(image_patches2*output_kernels2,axis = -2),axis = -2)\n",
        "    \n",
        "    output_image = output_images1+output_images2\n",
        "\n",
        "\n",
        "    \n",
        "    return tf.keras.Model(inputs = x_input, outputs = output_image, name = 'IIASC')\n",
        "    \n",
        "#     x = self.conv32(x)\n",
        "#     x = self.pool(x)\n",
        "\n",
        "#     x64 = self.conv64(x)\n",
        "#     x128 = self.pool(x64)\n",
        "\n",
        "#     x128 = self.conv128(x128)\n",
        "#     x256 = self.pool(x128)\n",
        "\n",
        "#     x256 = self.conv256(x256)\n",
        "#     x512 = self.pool(x256)\n",
        "\n",
        "#     x512 = self.conv512(x512)\n",
        "#     x = self.pool(x512)\n",
        "\n",
        "#     x = self.conv512x512(x)\n",
        "\n",
        "#     # ------------ Expansion ------------\n",
        "\n",
        "#     x = self.upsamp512(x)\n",
        "#     x += x512\n",
        "#     x = self.upconv256(x)\n",
        "\n",
        "#     x = self.upsamp256(x)\n",
        "#     x += x256\n",
        "#     x = self.upconv128(x)\n",
        "\n",
        "#     x = self.upsamp128(x)\n",
        "#     x += x128\n",
        "#     x = self.upconv64(x)\n",
        "\n",
        "#     x = self.upsamp64(x)\n",
        "#     x += x64\n",
        "\n",
        "#     # ------------ Final branches ------------\n",
        "\n",
        "#     k2h = self.upconv51_1(x)\n",
        "\n",
        "#     k2v = self.upconv51_2(x)\n",
        "\n",
        "#     k1h = self.upconv51_3(x)\n",
        "\n",
        "#     k1v = self.upconv51_4(x)\n",
        "\n",
        "#     padded_i2 = self.pad(i2)\n",
        "#     padded_i1 = self.pad(i1)\n",
        "#     self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hQTuMXQDl5Bo",
        "colab_type": "code",
        "outputId": "adf5469e-0f3a-4efa-b518-5d15bf457848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XQEFp-kvM8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6H9Ysq9l5Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PredictionCallback(tf.keras.callbacks.Callback):    \n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    y_pred = self.model.predict(img1)\n",
        "    plt.figure()\n",
        "    plt.imshow(image2[0,:,:,:])\n",
        "    plt.figure()\n",
        "    plt.imshow(y_pred[0,:,:,:])\n",
        "    plt.figure()\n",
        "    plt.imshow(image2[1,:,:,:])\n",
        "    plt.figure()\n",
        "    plt.imshow(y_pred[1,:,:,:])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLH3kFcZ5stS",
        "colab_type": "code",
        "outputId": "65a4bad9-5a54-4724-c5e7-e35b6498e276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "  y_pred = loaded_model.predict(img1)\n",
        "  plt.figure()\n",
        "  plt.imshow(image2[0,:,:,:])\n",
        "  plt.figure()\n",
        "  plt.imshow(y_pred[0,:,:,:])\n",
        "  plt.figure()\n",
        "  plt.imshow(image2[1,:,:,:])\n",
        "  plt.figure()\n",
        "  plt.imshow(y_pred[1,:,:,:])\n",
        "  plt.show()\n",
        "  # print(y_pred[0,:,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ykNJumtUG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f782c30b-4a46-4f55-e0a0-fee5dbd38857"
      },
      "source": [
        "json_file = open('model (1).json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model (1).h5\")\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8eQAiwVl5Bv",
        "colab_type": "code",
        "outputId": "b953573e-c35a-4b15-f68a-fcc4eee6fe08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# with strategy.scope():\n",
        "\n",
        "    \n",
        "#     model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.Conv2D(),\n",
        "#         tf.keras.layers.GlobalAveragePooling2D(),\n",
        "#         tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
        "#     ])\n",
        "        \n",
        "loaded_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(clipvalue = 1),\n",
        "    loss = custom_loss,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48nF-_g9tO4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlglIsOLfbY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR_START = 0.000005\n",
        "LR_MAX = 0.0001\n",
        "LR_MIN = 0.000005\n",
        "LR_RAMPUP_EPOCHS = 3\n",
        "LR_SUSTAIN_EPOCHS = 3\n",
        "LR_EXP_DECAY = .8\n",
        "\n",
        "def lrfn(epoch):\n",
        "    if epoch+7 < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch+9 < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "         lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS+9) + LR_MIN\n",
        "    return lr\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDOilMmcJZBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "40fe6b81-3e43-4116-d984-94effe28493c"
      },
      "source": [
        "print(training_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7wp86h_l5B0",
        "colab_type": "code",
        "outputId": "32b5654c-5025-458e-dee5-912a9454dc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = loaded_model.fit(get_training_dataset(training_filenames), steps_per_epoch=steps_per_epochs, epochs=4,callbacks = [PredictionCallback(),lr_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRSOSX3Bsm5r",
        "colab_type": "code",
        "outputId": "3a318d9d-576b-4c3c-b720-de67f70f00eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEGTt_fvoFiP",
        "colab_type": "code",
        "outputId": "a7adf3de-e26e-40ed-df9b-ffc74b1b7d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for img1,img2 in get_training_dataset(training_filenames).take(1):\n",
        "      print(img1.numpy().shape, img2.numpy().shape)\n",
        "image1 = img1.numpy()[:,:,:,0:3]\n",
        "image2 = img2.numpy()\n",
        "image3 = img1.numpy()[:,:,:,3:6]\n",
        "plt.figure()\n",
        "plt.imshow(image1[0,:,:,:])\n",
        "plt.figure()\n",
        "plt.imshow(image2[0,:,:,:])\n",
        "plt.figure()\n",
        "plt.imshow(image3[0,:,:,:])\n",
        "plt.show()\n",
        "# y_pred = model.predict(img1)\n",
        "# plt.figure()\n",
        "y_pred = loaded_model.predict(img1)\n",
        "\n",
        "plt.imshow(image2[0,:,:,:])\n",
        "plt.figure()\n",
        "plt.imshow(y_pred[0,:,:,:])\n",
        "plt.figure()\n",
        "plt.imshow(image2[1,:,:,:])\n",
        "plt.figure()\n",
        "plt.imshow(y_pred[1,:,:,:])\n",
        "plt.show()\n",
        "# print(y_pred[0,:,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "model_working5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}